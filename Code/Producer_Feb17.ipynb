{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a3e65e0",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "Tamir Shoresh\n",
    "\n",
    "Dan Goren\n",
    "\n",
    "Liora Cohen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a597e9a",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "1. Most clustering methods have been developed under the assumption of a static database. In modern applications, the database is usually updated in real-time. Shkapsky et al, 2016, deals with the issues arising from utilizing known clustering methods on real-time data. \n",
    "\n",
    "2. Algorithms that were originally designed for a sequential nature, must often be redesigned to effectively use the distributed computational resources. Iaani et al, 2018, presents the CLUBS+ algorithm for efficient parallel clustering.\n",
    "\n",
    "3. Bisecting k-means is a hierarchal k-means algorithm. The bisecting steps of clusters on the same level are grouped together to increase parallelism (Abirami et al, 2016).\n",
    "\n",
    "\n",
    "# Dataset\n",
    "\n",
    "The Customer Classification dataset is a set of customer data including:\n",
    "   1. region of store\n",
    "   2. tenure\n",
    "   3. age\n",
    "   4. income\n",
    "   5. marital status\n",
    "   6. address\n",
    "   7. education group\n",
    "   8. employment years\n",
    "   9. retirement status\n",
    "   10. gender\n",
    "   11. residential area code\n",
    "   12. customer catagory (target variable)\n",
    "    \n",
    "\n",
    "# Methods\n",
    "In this work, we wish to create a Kafka server to mimic the properties of real-time data, just as in Shkapsky et al, 2016. This will be done by creating a producer that reads from a csv file 100 lines and publishes them to the topic \"data\". we will also add a 5 second sleep between publishes. On the consumer side, we will read the data from the topic into a Spark RDD.\n",
    "Next, we would like to compare 2 clustering methods. The first is k-means, which does not utilize spark in an efficient maner. The second method is bisecting k-means, which utilizes spark more efficiently, but due to its algorithm may have less accuracy. we will compare them both on calculation time and on accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe4fe690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created topic data.\r\n"
     ]
    }
   ],
   "source": [
    "# creating the topic \"data\" in port 9092\n",
    "!/usr/local/kafka/kafka_2.13-3.2.1/bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca75f4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "%4|1675706926.377|TERMINATE|rdkafka#producer-4| [thrd:app]: Producer terminating with 15 messages (200229 bytes) still in queue or transit: use flush() to wait for outstanding message delivery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['657', '0', '2.5', '0', '0', '0', '37', '0.7', '141', '2', '1', '476', '961', '1412', '14', '3', '18', '0', '1', '0', '0']\n",
      "Finished 100\n",
      "['609', '0', '0.5', '0', '3', '0', '26', '0.3', '93', '4', '4', '938', '1948', '1866', '11', '10', '14', '1', '1', '1', '1']\n",
      "Finished 200\n",
      "['928', '1', '0.5', '1', '11', '0', '56', '0.7', '80', '8', '13', '221', '1243', '666', '19', '10', '5', '0', '1', '0', '0']\n",
      "Finished 300\n",
      "['1323', '1', '2.5', '1', '10', '1', '28', '0.2', '131', '4', '12', '162', '619', '1892', '10', '0', '15', '1', '1', '1', '1']\n",
      "Finished 400\n",
      "['1926', '1', '1.7', '1', '1', '1', '33', '0.4', '172', '3', '2', '982', '1157', '2488', '6', '1', '14', '1', '0', '0', '2']\n",
      "Finished 500\n",
      "['1137', '0', '2.2', '0', '6', '1', '3', '0.9', '173', '5', '7', '1250', '1285', '316', '15', '14', '17', '1', '0', '0', '0']\n",
      "Finished 600\n",
      "['1940', '1', '0.9', '1', '4', '0', '17', '0.7', '93', '5', '7', '112', '858', '2297', '14', '6', '8', '0', '1', '0', '2']\n",
      "Finished 700\n",
      "['1800', '1', '0.7', '0', '2', '1', '8', '0.4', '100', '7', '6', '400', '823', '3481', '10', '8', '8', '1', '0', '1', '3']\n",
      "Finished 800\n",
      "['1372', '1', '2.7', '0', '7', '0', '34', '0.4', '193', '4', '17', '687', '937', '725', '11', '3', '20', '1', '0', '0', '0']\n",
      "Finished 900\n",
      "['1373', '1', '1.9', '1', '1', '1', '29', '0.9', '141', '6', '12', '1220', '1348', '2752', '15', '2', '7', '1', '1', '1', '3']\n",
      "Finished 1000\n",
      "['1827', '1', '0.8', '0', '4', '0', '61', '0.9', '130', '7', '9', '253', '1764', '2693', '8', '3', '7', '0', '1', '1', '3']\n",
      "Finished 1100\n",
      "['708', '0', '1.9', '0', '0', '0', '26', '0.3', '116', '7', '1', '206', '620', '1419', '10', '6', '16', '1', '1', '1', '0']\n",
      "Finished 1200\n",
      "['1089', '0', '1.4', '1', '10', '0', '2', '0.1', '153', '7', '20', '211', '1409', '2620', '7', '5', '13', '1', '0', '0', '2']\n",
      "Finished 1300\n",
      "['1540', '0', '0.7', '1', '0', '1', '29', '0.1', '157', '7', '1', '318', '831', '1161', '11', '8', '5', '1', '0', '0', '0']\n",
      "Finished 1400\n",
      "['881', '0', '1.0', '0', '1', '0', '64', '0.5', '160', '3', '6', '115', '636', '2110', '18', '9', '6', '1', '1', '1', '1']\n",
      "Finished 1500\n",
      "['1851', '0', '2.9', '0', '0', '0', '53', '0.7', '112', '6', '2', '174', '1175', '2678', '6', '0', '8', '1', '0', '0', '2']\n",
      "Finished 1600\n",
      "['1619', '1', '0.9', '0', '3', '0', '20', '0.9', '117', '6', '9', '813', '1180', '464', '7', '5', '16', '1', '0', '1', '0']\n",
      "Finished 1700\n",
      "['722', '0', '1.1', '0', '12', '0', '12', '1.0', '93', '6', '20', '138', '1371', '2359', '13', '6', '3', '0', '0', '0', '1']\n",
      "Finished 1800\n",
      "['684', '1', '0.9', '1', '3', '1', '63', '1.0', '157', '5', '9', '159', '1738', '3756', '17', '5', '12', '1', '1', '1', '3']\n",
      "Finished 1900\n",
      "['1512', '0', '0.9', '0', '4', '1', '46', '0.1', '145', '5', '5', '336', '670', '869', '18', '10', '19', '1', '1', '1', '0']\n",
      "Finished 2000\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from confluent_kafka import Producer\n",
    "import time\n",
    "import csv\n",
    "import json\n",
    "import numpy as np\n",
    "# create producer\n",
    "p = Producer({'bootstrap.servers': 'localhost:9092'})\n",
    "# open csv file\n",
    "i=0\n",
    "with open('train.csv') as f:\n",
    "    row_count = 0\n",
    "    for _ in f:\n",
    "        row_count +=1\n",
    "# run producer in infinite loop\n",
    "while True: \n",
    "    flag = 0\n",
    "    # create dictionary of features that contains words as keys and numbers as values\n",
    "    d={}\n",
    "    if flag == 0 :\n",
    "        # Open file \n",
    "        file = open('train.csv')\n",
    "        reader_obj = csv.reader(file)\n",
    "        \n",
    "        for row in reader_obj:\n",
    "            if i<row_count+1:\n",
    "                # adding a row to the dictionary\n",
    "                d[i] = row\n",
    "                i += 1                \n",
    "                # sleep for 5 seconds\n",
    "                if i % 100 == 0:\n",
    "                    print(row)\n",
    "                    # every 100 lines (customers), encode the dictionary (for publishing) and send it through the producer\n",
    "                    d_encoded = json.dumps(d).encode('utf-8')\n",
    "                    p.produce('data', value=d_encoded)\n",
    "                    time.sleep(5)\n",
    "                    # reset dictionary\n",
    "                    d={}\n",
    "                    print(\"Finished \" + str(i))\n",
    "            else: \n",
    "                flag = 1\n",
    "                break     \n",
    "            \n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce586251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f127b174",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
